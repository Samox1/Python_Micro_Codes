HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(8,8), lr = 0.1, iter = 5000, seed = 333, type = 'reg')
source("funkcje.R")
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(8,8), lr = 0.1, iter = 5000, seed = 333, type = 'reg')
predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg')
summary(regresja_NN)
View(HELL_3)
HELL_3[["Wagi"]]
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(8,8), lr = 0.001, iter = 2, seed = 333, type = 'reg')
source("funkcje.R")
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(8,8), lr = 0.001, iter = 2, seed = 333, type = 'reg')
source("funkcje.R")
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(8,8), lr = 0.001, iter = 2, seed = 333, type = 'reg')
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(8,8), lr = 0.001, iter = 4, seed = 333, type = 'reg')
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(8,8), lr = 0.001, iter = 8, seed = 333, type = 'reg')
predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg')
ModelOcena((regresja_NN[,regresja_Y]), predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg'))
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(8,8), lr = 0.001, iter = 50, seed = 333, type = 'reg')
predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg')
ModelOcena((regresja_NN[,regresja_Y]), predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg'))
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(8,8), lr = 0.001, iter = 500, seed = 333, type = 'reg')
source("funkcje.R")
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(5,5), lr = 0.001, iter = 500, seed = 333, type = 'reg')
predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg')
ModelOcena((regresja_NN[,regresja_Y]), predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg'))
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(5,5), lr = 0.001, iter = 5000, seed = 333, type = 'reg')
predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg')
ModelOcena((regresja_NN[,regresja_Y]), predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg'))
HELL_3 <- trainNN(regresja_Y, regresja_X, regresja_NN, h = c(5,5), lr = 0.001, iter = 50000, seed = 333, type = 'reg')
predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg')
ModelOcena((regresja_NN[,regresja_Y]), predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg'))
summary(regresja_NN)
predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg')
ModelOcena((regresja[,regresja_Y]), MinMaxOdwrot(predNN(as.matrix(regresja_NN[,regresja_X]), HELL_3, type = 'reg'), y_min = min(regresja[,regresja_Y]), y_max = max(regresja[,regresja_Y])))
ModelOcena((wieloklasowa[,wieloklasowa_Y]), predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')[,'Class'])
HELL_2 <- trainNN(wieloklasowa_Y, wieloklasowa_X, wieloklasowa_NN, h = c(10,10), lr = 0.1, iter = 50000, seed = 333, type = 'multi')
predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')
summary(predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi'))
ModelOcena((wieloklasowa[,wieloklasowa_Y]), predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')[,'Class'])
HELL_2 <- trainNN(wieloklasowa_Y, wieloklasowa_X, wieloklasowa_NN, h = c(12,12), lr = 0.5, iter = 10000, seed = 333, type = 'multi')
summary(predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi'))
ModelOcena((wieloklasowa[,wieloklasowa_Y]), predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')[,'Class'])
HELL_2 <- trainNN(wieloklasowa_Y, wieloklasowa_X, wieloklasowa_NN, h = c(8,8), lr = 0.5, iter = 10000, seed = 333, type = 'multi')
summary(predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi'))
ModelOcena((wieloklasowa[,wieloklasowa_Y]), predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')[,'Class'])
HELL_2 <- trainNN(wieloklasowa_Y, wieloklasowa_X, wieloklasowa_NN, h = c(8,6), lr = 0.5, iter = 10000, seed = 333, type = 'multi')
ModelOcena((wieloklasowa[,wieloklasowa_Y]), predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')[,'Class'])
HELL_2 <- trainNN(wieloklasowa_Y, wieloklasowa_X, wieloklasowa_NN, h = c(8,8), lr = 0.5, iter = 30000, seed = 333, type = 'multi')
# predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')
# summary(predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi'))
ModelOcena((wieloklasowa[,wieloklasowa_Y]), predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')[,'Class'])
HELL_2 <- trainNN(wieloklasowa_Y, wieloklasowa_X, wieloklasowa_NN, h = c(8,8), lr = 0.9, iter = 30000, seed = 333, type = 'multi')
# predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')
# summary(predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi'))
ModelOcena((wieloklasowa[,wieloklasowa_Y]), predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')[,'Class'])
HELL_2 <- trainNN(wieloklasowa_Y, wieloklasowa_X, wieloklasowa_NN, h = c(8,8), lr = 1.5, iter = 30000, seed = 333, type = 'multi')
# predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')
# summary(predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi'))
ModelOcena((wieloklasowa[,wieloklasowa_Y]), predNN(as.matrix(wieloklasowa_NN[,wieloklasowa_X]), HELL_2, type = 'multi')[,'Class'])
HELL_2 <- trainNN(wieloklasowa_Y, wieloklasowa_X, wieloklasowa_NN, h = c(8,8), lr = 0.9, iter = 80000, seed = 333, type = 'multi')
source("funkcje.R")
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 0.9), iter = c(20000, 80000))
parTune_NN_bin
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
?expand.grid
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
View(parTune_NN_bin)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
source("funkcje.R")
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 0.9), iter = c(20000, 80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
ramka <- as.data.frame(expand_grid(k_=c(1:kFold), parTune))
source("funkcje.R")
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(80000))
source("funkcje.R")
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
binarna_NN <- binarna
binarna_NN <- as.data.frame(sapply(binarna_NN, as.numeric))
binarna_NN <- as.data.frame(sapply(binarna_NN, MinMax))
HELL <- trainNN(binarna_Y, binarna_X, binarna_NN, h = c(4,4), lr = 0.01, iter = 2000, seed = 333, type = 'bin')
# predNN(binarna_NN[,binarna_X], HELL, type = 'bin')
ModelOcena(as.factor(binarna_NN[,binarna_Y]), predNN(binarna_NN[,binarna_X], HELL, type = 'bin'))
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
View(parTune_NN_bin)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)
source("funkcje.R")
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)
parTune_NN_multi <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
source("funkcje.R")
parTune_NN_multi <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)
source("funkcje.R")
parTune_NN_multi <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)
source("funkcje.R")
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)
source("funkcje.R")
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)
source("funkcje.R")
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)
source("funkcje.R")
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)
# parTune_NN_multi <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
parTune_NN_multi <- expand.grid(h = list(c(8,8)), lr = c(1.0), iter = c(150000))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)
parTune_Tree_multi <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.05, 0.022))
View(parTune_Tree_multi)
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)
source("funkcje.R")
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)
source("funkcje.R")
parTune_Tree_multi <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.05, 0.022))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)
source("funkcje.R")
parTune_Tree_bin <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.02))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)
# Tree_3 <- Tree(regresja_Y, regresja_X, regresja, type = "SS", depth = 6, minobs = 2, overfit = 'none', cf = 0.2)
# Tree_wynik_3 <- PredictTree(Tree_3, regresja[,regresja_X])
# ModelOcena(regresja[,regresja_Y], (Tree_wynik_3))
source("funkcje.R")
parTune_Tree_bin <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.02))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)
# Tree_3 <- Tree(regresja_Y, regresja_X, regresja, type = "SS", depth = 6, minobs = 2, overfit = 'none', cf = 0.2)
# Tree_wynik_3 <- PredictTree(Tree_3, regresja[,regresja_X])
# ModelOcena(regresja[,regresja_Y], (Tree_wynik_3))
source("funkcje.R")
parTune_Tree_reg <- expand.grid(type = c('SS'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('none'), cf = c(0.02))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'Tree', kFold = 10, parTune = parTune_Tree_reg, seed = 333)
# Tree_3 <- Tree(regresja_Y, regresja_X, regresja, type = "SS", depth = 6, minobs = 2, overfit = 'none', cf = 0.2)
# Tree_wynik_3 <- PredictTree(Tree_3, regresja[,regresja_X])
# ModelOcena(regresja[,regresja_Y], (Tree_wynik_3))
source("funkcje.R")
parTune_Tree_reg <- expand.grid(type = c('SS'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('none'), cf = c(0.02))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'Tree', kFold = 10, parTune = parTune_Tree_reg, seed = 333)
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_reg <- expand.model.frame(k = c(2:20))
parTune_KNN_reg <- expand.grid(k = c(2:20))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)[1:5,]
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_bin <- expand.grid(k = c(2:20))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'KNN', kFold = 10, parTune = parTune_KNN_bin, seed = 333)[1:5,]
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_reg <- expand.grid(k = c(2:20))
parTune_KNN_bin <- expand.grid(k = c(2:20))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'KNN', kFold = 10, parTune = parTune_KNN_bin, seed = 333)[1:5,]
warnings()
KNN_Model <- KNNtrain(binarna[,binarna_X], binarna[,binarna_Y], k=2, 0, 1)
KNN_wynik <- KNNpred(KNNmodel = KNN_Model, binarna[,binarna_X], Ncores = 10)
KNN_Model <- KNNtrain(binarna[,binarna_X], binarna[,binarna_Y], k=2, 0, 1)
KNN_wynik <- KNNpred(KNNmodel = KNN_Model, binarna[5:10,binarna_X], Ncores = 10)
View(KNN_Model)
KNN_Model[["y_tar"]]
KNN_Model <- KNNtrain(binarna[,binarna_X], binarna[,binarna_Y], k=2, 0, 1)
KNN_wynik <- KNNpred(KNNmodel = KNN_Model, binarna[5:10,binarna_X], Ncores = 10)
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
KNN_Model <- KNNtrain(binarna[,binarna_X], binarna[,binarna_Y], k=2, 0, 1)
KNN_wynik <- KNNpred(KNNmodel = KNN_Model, binarna[5:10,binarna_X], Ncores = 10)
View(KNN_wynik)
ModelOcena(binarna[5:10,binarna_Y], KNN_wynik[,2])
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_bin <- expand.grid(k = c(2:20))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'KNN', kFold = 10, parTune = parTune_KNN_bin, seed = 333)[1:5,]
View(KNN_wynik)
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_multi <- expand.grid(k = c(2:20))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'KNN', kFold = 10, parTune = parTune_KNN_multi, seed = 333)[1:5,]
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_multi <- expand.grid(k = c(2:20))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'KNN', kFold = 10, parTune = parTune_KNN_multi, seed = 333)[1:5,]
# Tree_3 <- Tree(regresja_Y, regresja_X, regresja, type = "SS", depth = 6, minobs = 2, overfit = 'none', cf = 0.2)
# Tree_wynik_3 <- PredictTree(Tree_3, regresja[,regresja_X])
# ModelOcena(regresja[,regresja_Y], (Tree_wynik_3))
source("funkcje.R")
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)[1:5,]
parTune_NN_multi <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)[1:5,]
parTune_NN_multi <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)[1:5,]
parTune_Tree_reg <- expand.grid(type = c('SS'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('none'), cf = c(0.02))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'Tree', kFold = 10, parTune = parTune_Tree_reg, seed = 333)[1:5,]
parTune_Tree_bin <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.02))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'Tree', kFold = 10, parTune = parTune_Tree_bin, seed = 333)[1:5,]
parTune_Tree_multi <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.05, 0.022))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)[1:5,]
parTune_KNN_reg <- expand.grid(k = c(2:20))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)[1:5,]
parTune_KNN_bin <- expand.grid(k = c(2:20))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'KNN', kFold = 10, parTune = parTune_KNN_bin, seed = 333)[1:5,]
parTune_KNN_multi <- expand.grid(k = c(2:20))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'KNN', kFold = 10, parTune = parTune_KNN_multi, seed = 333)[1:5,]
warnings()
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
View(CV_NN_reg)
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
parTune_KNN_reg <- expand.grid(k = c(2:20))
CV_KNN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)
parTune_KNN_reg <- expand.grid(k = c(2:15))
CV_KNN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
parTune_KNN_reg <- expand.grid(k = c(2:15))
CV_KNN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
