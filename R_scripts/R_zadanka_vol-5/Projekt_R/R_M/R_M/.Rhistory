parTune_Tree_multi <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.05, 0.022))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)
source("funkcje.R")
parTune_Tree_bin <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.02))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)
# Tree_3 <- Tree(regresja_Y, regresja_X, regresja, type = "SS", depth = 6, minobs = 2, overfit = 'none', cf = 0.2)
# Tree_wynik_3 <- PredictTree(Tree_3, regresja[,regresja_X])
# ModelOcena(regresja[,regresja_Y], (Tree_wynik_3))
source("funkcje.R")
parTune_Tree_bin <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.02))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)
# Tree_3 <- Tree(regresja_Y, regresja_X, regresja, type = "SS", depth = 6, minobs = 2, overfit = 'none', cf = 0.2)
# Tree_wynik_3 <- PredictTree(Tree_3, regresja[,regresja_X])
# ModelOcena(regresja[,regresja_Y], (Tree_wynik_3))
source("funkcje.R")
parTune_Tree_reg <- expand.grid(type = c('SS'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('none'), cf = c(0.02))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'Tree', kFold = 10, parTune = parTune_Tree_reg, seed = 333)
# Tree_3 <- Tree(regresja_Y, regresja_X, regresja, type = "SS", depth = 6, minobs = 2, overfit = 'none', cf = 0.2)
# Tree_wynik_3 <- PredictTree(Tree_3, regresja[,regresja_X])
# ModelOcena(regresja[,regresja_Y], (Tree_wynik_3))
source("funkcje.R")
parTune_Tree_reg <- expand.grid(type = c('SS'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('none'), cf = c(0.02))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'Tree', kFold = 10, parTune = parTune_Tree_reg, seed = 333)
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_reg <- expand.model.frame(k = c(2:20))
parTune_KNN_reg <- expand.grid(k = c(2:20))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)[1:5,]
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_bin <- expand.grid(k = c(2:20))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'KNN', kFold = 10, parTune = parTune_KNN_bin, seed = 333)[1:5,]
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_reg <- expand.grid(k = c(2:20))
parTune_KNN_bin <- expand.grid(k = c(2:20))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'KNN', kFold = 10, parTune = parTune_KNN_bin, seed = 333)[1:5,]
warnings()
KNN_Model <- KNNtrain(binarna[,binarna_X], binarna[,binarna_Y], k=2, 0, 1)
KNN_wynik <- KNNpred(KNNmodel = KNN_Model, binarna[,binarna_X], Ncores = 10)
KNN_Model <- KNNtrain(binarna[,binarna_X], binarna[,binarna_Y], k=2, 0, 1)
KNN_wynik <- KNNpred(KNNmodel = KNN_Model, binarna[5:10,binarna_X], Ncores = 10)
View(KNN_Model)
KNN_Model[["y_tar"]]
KNN_Model <- KNNtrain(binarna[,binarna_X], binarna[,binarna_Y], k=2, 0, 1)
KNN_wynik <- KNNpred(KNNmodel = KNN_Model, binarna[5:10,binarna_X], Ncores = 10)
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
KNN_Model <- KNNtrain(binarna[,binarna_X], binarna[,binarna_Y], k=2, 0, 1)
KNN_wynik <- KNNpred(KNNmodel = KNN_Model, binarna[5:10,binarna_X], Ncores = 10)
View(KNN_wynik)
ModelOcena(binarna[5:10,binarna_Y], KNN_wynik[,2])
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_bin <- expand.grid(k = c(2:20))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'KNN', kFold = 10, parTune = parTune_KNN_bin, seed = 333)[1:5,]
View(KNN_wynik)
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_multi <- expand.grid(k = c(2:20))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'KNN', kFold = 10, parTune = parTune_KNN_multi, seed = 333)[1:5,]
# KNN_Model_multi <- KNNtrain(wieloklasowa[,wieloklasowa_X], wieloklasowa[,wieloklasowa_Y], k=2, 0, 1)
# KNN_wynik_multi <- KNNpred(KNNmodel = KNN_Model_multi, wieloklasowa[,wieloklasowa_X], Ncores = 20)
# ModelOcena(wieloklasowa[,wieloklasowa_Y], KNN_wynik_multi[,length(KNN_wynik_multi)])
source("funkcje.R")
parTune_KNN_multi <- expand.grid(k = c(2:20))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'KNN', kFold = 10, parTune = parTune_KNN_multi, seed = 333)[1:5,]
# Tree_3 <- Tree(regresja_Y, regresja_X, regresja, type = "SS", depth = 6, minobs = 2, overfit = 'none', cf = 0.2)
# Tree_wynik_3 <- PredictTree(Tree_3, regresja[,regresja_X])
# ModelOcena(regresja[,regresja_Y], (Tree_wynik_3))
source("funkcje.R")
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)[1:5,]
parTune_NN_multi <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
parTune_NN_bin <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'NN', kFold = 10, parTune = parTune_NN_bin, seed = 333)[1:5,]
parTune_NN_multi <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'NN', kFold = 10, parTune = parTune_NN_multi, seed = 333)[1:5,]
parTune_Tree_reg <- expand.grid(type = c('SS'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('none'), cf = c(0.02))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'Tree', kFold = 10, parTune = parTune_Tree_reg, seed = 333)[1:5,]
parTune_Tree_bin <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.02))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'Tree', kFold = 10, parTune = parTune_Tree_bin, seed = 333)[1:5,]
parTune_Tree_multi <- expand.grid(type = c('Entropy', 'Gini'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('prune', 'none'), cf = c(0.05, 0.022))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'Tree', kFold = 10, parTune = parTune_Tree_multi, seed = 333)[1:5,]
parTune_KNN_reg <- expand.grid(k = c(2:20))
CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)[1:5,]
parTune_KNN_bin <- expand.grid(k = c(2:20))
CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'KNN', kFold = 10, parTune = parTune_KNN_bin, seed = 333)[1:5,]
parTune_KNN_multi <- expand.grid(k = c(2:20))
CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'KNN', kFold = 10, parTune = parTune_KNN_multi, seed = 333)[1:5,]
warnings()
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
source("funkcje.R")
parTune_NN_reg <- expand.grid(h = list(c(8,8), c(8,8,8)), lr = c(0.01, 0.1, 1.0), iter = c(20000, 80000))
CV_NN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'NN', kFold = 10, parTune = parTune_NN_reg, seed = 333)[1:5,]
View(CV_NN_reg)
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
parTune_KNN_reg <- expand.grid(k = c(2:20))
CV_KNN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)
parTune_KNN_reg <- expand.grid(k = c(2:15))
CV_KNN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
parTune_KNN_reg <- expand.grid(k = c(2:15))
CV_KNN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
source("funkcje.R")
print("******************************************************")
print("*** Wczytywanie danych do regresji - Computer Hardware")                                         # https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
regresja <- read.csv("machine.data", header = FALSE)[,-10]                                              # Ostatnia kolumna wedlug notki to estymacja wydajnosci (kolumny nr 9) z jakiegos artykulu
regresja[,1] <- as.factor(regresja[,1])
regresja <- regresja[,-2]
regresja_Y <- colnames(regresja)[ncol(regresja)]
regresja_X <- colnames(regresja)[-ncol(regresja)]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(regresja)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(regresja))
print("******************************************************")
print("*** Wczytywanie danych do klasyfikacji binarnej - Breast Cancer Wisconsin (Diagnostic)")         # http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
binarna <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
binarna[,11] <- as.factor(binarna[,11])                                                                 # Klasy zapisane jako wartosci numeryczne -> "2 for benign, 4 for malignant" (z notki)
binarna <- subset(binarna, V7 != '?')
binarna[,7] <- as.numeric(binarna[,7])
binarna_Y <- colnames(binarna)[11]
binarna_X <- colnames(binarna)[-11]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(binarna)))
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(binarna))
print("******************************************************")
print("** Wczytywanie danych do klasyfikacji wieloklasowej - Yeast")                                    # Dane od prowadzacego -> https://archive.ics.uci.edu/ml/datasets/Yeast
wieloklasowa <- read.csv("yeast.data", header = FALSE, sep = ";")[,-1]                                  # Dane wedlug notki maja 8 kolumn numerycznych i 1 z klasami docelowymi -> kolumna nr 1 to nic nie mowiaca nazwa
wieloklasowa[,9] <- as.factor(wieloklasowa[,9])
wieloklasowa_Y <- colnames(wieloklasowa)[9]
wieloklasowa_X <- colnames(wieloklasowa)[-9]
print(paste0("Czy dane do klasyfikacji wieloklasowej maja warstosci NA: ", anyNA(wieloklasowa)))        # Dane mialy specyficzny separator, ktory zostal zamieniony na srednik + upewniono sie ze nie ma zadnych problemow z separatorem
print("Podsumowanie danych w kazdej kolumnie:")
print(summary(wieloklasowa))
print("Rozklad klas w danych: ")
print(sort(summary(wieloklasowa[,9])))
parTune_KNN_reg <- expand.grid(k = c(2:15))
CV_KNN_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'KNN', kFold = 10, parTune = parTune_KNN_reg, seed = 333)
View(CV_KNN_reg)
parTune_KNN_bin <- expand.grid(k = c(2:15))
CV_KNN_bin <- CrossValidTune(binarna_Y, binarna_X, binarna, algo = 'KNN', kFold = 10, parTune = parTune_KNN_bin, seed = 333)
View(CV_KNN_bin)
parTune_KNN_multi <- expand.grid(k = c(2:15))
CV_KNN_multi <- CrossValidTune(wieloklasowa_Y, wieloklasowa_X, wieloklasowa, algo = 'KNN', kFold = 10, parTune = parTune_KNN_multi, seed = 333)
View(CV_KNN_reg)
View(CV_KNN_multi)
View(CV_KNN_bin)
KNN_reg_CrossValid_gr <- CV_KNN_reg %>% group_by(k)
KNN_reg_CrossValid_gr <- as.data.frame(KNN_reg_CrossValid_gr %>% summarise(MAET = mean(MAET), MSET = mean(MSET), MAPET = mean(MAPET), MAEW = mean(MAEW), MSEW = mean(MSEW), MAPEW = mean(MAPEW)))
print(KNN_reg_CrossValid_gr)
KNN_reg_best_T <- KNN_reg_CrossValid_gr[which.min(KNN_reg_CrossValid_gr$MAET),]
KNN_reg_best_W <- KNN_reg_CrossValid_gr[which.min(KNN_reg_CrossValid_gr$MAEW),]
print(KNN_reg_best_T)
print(KNN_reg_best_W)
KNN_bin_CrossValid_gr <- CV_KNN_bin %>% group_by(k)
KNN_bin_CrossValid_gr <- as.data.frame(KNN_bin_CrossValid_gr %>% summarise(AUCT = mean(AUCT), CzuloscT = mean(CzuloscT), SpecyficznoscT = mean(SpecyficznoscT), JakoscT = mean(JakoscT),
AUCW = mean(AUCW), CzuloscW = mean(CzuloscW), SpecyficznoscW = mean(SpecyficznoscW), JakoscW = mean(JakoscW), ))
print(KNN_bin_CrossValid_gr)
KNN_bin_best_T <- KNN_bin_CrossValid_gr[which.max(KNN_bin_CrossValid_gr$JakoscT),]
KNN_bin_best_W <- KNN_bin_CrossValid_gr[which.max(KNN_bin_CrossValid_gr$JakoscW),]
print(KNN_bin_best_T)
print(KNN_bin_best_W)
KNN_multi_CrossValid_gr <- CV_KNN_multi %>% group_by(k)
KNN_multi_CrossValid_gr <- as.data.frame(KNN_multi_CrossValid_gr %>% summarise(ACCT = mean(ACCT), ACCW = mean(ACCW)))
print(KNN_multi_CrossValid_gr)
KNN_multi_CrossValid_gr <- CV_KNN_multi %>% group_by(k)
KNN_multi_CrossValid_gr <- as.data.frame(KNN_multi_CrossValid_gr %>% summarise(ACCT = mean(ACCT), ACCW = mean(ACCW)))
KNN_multi_CrossValid_gr <- CV_KNN_multi %>% group_by(k)
KNN_multi_CrossValid_gr <- as.data.frame(KNN_multi_CrossValid_gr %>% summarise(JakoscT = mean(JakoscT), JakoscW = mean(JakoscW)))
print(KNN_multi_CrossValid_gr)
KNN_multi_best_T <- KNN_multi_CrossValid_gr[which.max(KNN_multi_CrossValid_gr$ACCT),]
KNN_multi_best_W <- KNN_multi_CrossValid_gr[which.max(KNN_multi_CrossValid_gr$ACCW),]
print(KNN_multi_best_T)
print(KNN_multi_best_W)
KNN_multi_best_T <- KNN_multi_CrossValid_gr[which.max(KNN_multi_CrossValid_gr$JakoscT),]
KNN_multi_best_W <- KNN_multi_CrossValid_gr[which.max(KNN_multi_CrossValid_gr$JakoscW),]
print(KNN_multi_best_T)
print(KNN_multi_best_W)
View(KNN_bin_CrossValid_gr)
KNN_bin_CrossValid_gr <- CV_KNN_bin %>% group_by(k)
KNN_bin_CrossValid_gr[is.na(KNN_bin_CrossValid_gr)] <- 0
KNN_bin_CrossValid_gr <- as.data.frame(KNN_bin_CrossValid_gr %>% summarise(AUCT = mean(AUCT), CzuloscT = mean(CzuloscT), SpecyficznoscT = mean(SpecyficznoscT), JakoscT = mean(JakoscT),
AUCW = mean(AUCW), CzuloscW = mean(CzuloscW), SpecyficznoscW = mean(SpecyficznoscW), JakoscW = mean(JakoscW), ))
print(KNN_bin_CrossValid_gr)
KNN_bin_best_T <- KNN_bin_CrossValid_gr[which.max(KNN_bin_CrossValid_gr$JakoscT),]
KNN_bin_best_W <- KNN_bin_CrossValid_gr[which.max(KNN_bin_CrossValid_gr$JakoscW),]
print(KNN_bin_best_T)
print(KNN_bin_best_W)
KNN_reg_CrossValid_gr <- CV_KNN_reg %>% group_by(k)
KNN_reg_CrossValid_gr[is.na(KNN_reg_CrossValid_gr)] <- 0
KNN_reg_CrossValid_gr <- as.data.frame(KNN_reg_CrossValid_gr %>% summarise(MAET = mean(MAET), MSET = mean(MSET), MAPET = mean(MAPET), MAEW = mean(MAEW), MSEW = mean(MSEW), MAPEW = mean(MAPEW)))
print(KNN_reg_CrossValid_gr)
KNN_reg_best_T <- KNN_reg_CrossValid_gr[which.min(KNN_reg_CrossValid_gr$MAET),]
KNN_reg_best_W <- KNN_reg_CrossValid_gr[which.min(KNN_reg_CrossValid_gr$MAEW),]
print(KNN_reg_best_T)
print(KNN_reg_best_W)
KNN_bin_CrossValid_gr <- CV_KNN_bin %>% group_by(k)
KNN_bin_CrossValid_gr[is.na(KNN_bin_CrossValid_gr)] <- 0
KNN_bin_CrossValid_gr <- as.data.frame(KNN_bin_CrossValid_gr %>% summarise(AUCT = mean(AUCT), CzuloscT = mean(CzuloscT), SpecyficznoscT = mean(SpecyficznoscT), JakoscT = mean(JakoscT),
AUCW = mean(AUCW), CzuloscW = mean(CzuloscW), SpecyficznoscW = mean(SpecyficznoscW), JakoscW = mean(JakoscW), ))
print(KNN_bin_CrossValid_gr)
KNN_bin_best_T <- KNN_bin_CrossValid_gr[which.max(KNN_bin_CrossValid_gr$JakoscT),]
KNN_bin_best_W <- KNN_bin_CrossValid_gr[which.max(KNN_bin_CrossValid_gr$JakoscW),]
print(KNN_bin_best_T)
print(KNN_bin_best_W)
KNN_multi_CrossValid_gr <- CV_KNN_multi %>% group_by(k)
KNN_multi_CrossValid_gr[is.na(KNN_multi_CrossValid_gr)] <- 0
KNN_multi_CrossValid_gr <- as.data.frame(KNN_multi_CrossValid_gr %>% summarise(JakoscT = mean(JakoscT), JakoscW = mean(JakoscW)))
print(KNN_multi_CrossValid_gr)
KNN_multi_best_T <- KNN_multi_CrossValid_gr[which.max(KNN_multi_CrossValid_gr$JakoscT),]
KNN_multi_best_W <- KNN_multi_CrossValid_gr[which.max(KNN_multi_CrossValid_gr$JakoscW),]
print(KNN_multi_best_T)
print(KNN_multi_best_W)
View(KNN_bin_CrossValid_gr)
View(KNN_reg_CrossValid_gr)
View(KNN_multi_CrossValid_gr)
cv_R <- trainControl(method="cv", number=10)
?train
cv_R <- trainControl(method="cv", number=10)
binarna_R <- binarna
binarna_R[,7] <- as.numeric(as.character(as.numeric(binarna_R[,7])))
knn_grid_bin = expand.grid(k=2:20)
KNN_bin_R = train(x=binarna_R[,binarna_X], y=binarna_R[,binarna_Y], tuneGrid=knn_grid_bin, method='knn', metric='Accuracy', trControl=cv_R)
KNN_bin_R_Wynik = KNN_bin_R$results
print(paste("KNN w R - bin: k = ", KNN_bin_R$finalModel$k, " | Accuracy = " ,KNN_bin_R_Wynik$Accuracy[KNN_bin_R_Wynik$k == KNN_bin_R$finalModel$k]))
knn_grid_multi = expand.grid(k=2:20)
KNN_multi_R = train(x=wieloklasowa[,wieloklasowa_X], y=wieloklasowa[,wieloklasowa_Y], tuneGrid=knn_grid_multi, method='knn', metric='Accuracy', trControl=cv_R)
KNN_multi_R_Wynik = KNN_multi_R$results
print(paste("KNN w R - multi: k = ", KNN_multi_R$finalModel$k, " | Accuracy = " ,KNN_multi_R_Wynik$Accuracy[KNN_multi_R_Wynik$k == KNN_multi_R$finalModel$k]))
regresja_R <- regresja
regresja_R[,1] <- as.numeric(regresja_R[,1])
knn_grid_reg = expand.grid(k=2:20)
KNN_reg_R = train(x=regresja_R[,regresja_X], y=regresja_R[,regresja_Y], tuneGrid=knn_grid_reg, method='knn', metric='MAE', trControl=cv_R)
KNN_reg_R_Wynik = KNN_reg_R$results
print(paste("KNN w R - reg: k = ", KNN_reg_R$finalModel$k, " | MAE = " ,KNN_reg_R_Wynik$MAE[KNN_reg_R_Wynik$k == KNN_reg_R$finalModel$k]))
VS_KNN_Reg <- data.frame(k = c(1:15))
VS_KNN_Reg <- merge(VS_KNN_Reg, KNN_reg_CrossValid_gr[,c('k', "MAEW")], by = 'k')
VS_KNN_Reg <- merge(VS_KNN_Reg, KNN_reg_R_Wynik[,c('k', "MAE")], by = 'k')
print(VS_KNN_Reg)
ggplot(VS_KNN_Reg , aes(x=k)) + geom_line(aes(y = MAEW, color='blue'), size=1, ) + geom_line(aes(y = MAE, color='red'), size=1,) +
labs(title='KNN - Regresja: MAE od k', x='k', y='MAE') + scale_color_discrete(name = "Implementacja", labels = c("Wlasna", "Biblioteka R")) + theme(legend.position = "bottom")
View(KNN_reg_CrossValid_gr)
VS_KNN_Reg <- data.frame(k = c(1:15))
VS_KNN_Reg <- merge(VS_KNN_Reg, KNN_reg_CrossValid_gr[,c('k', "MAET")], by = 'k')
VS_KNN_Reg <- merge(VS_KNN_Reg, KNN_reg_CrossValid_gr[,c('k', "MAEW")], by = 'k')
VS_KNN_Reg <- merge(VS_KNN_Reg, KNN_reg_R_Wynik[,c('k', "MAE")], by = 'k')
print(VS_KNN_Reg)
ggplot(VS_KNN_Reg , aes(x=k)) + geom_line(aes(y = MAET, color='blue'), size=1, ) + geom_line(aes(y = MAEW, color='blue'), size=1, ) + geom_line(aes(y = MAE, color='red'), size=1,) +
labs(title='KNN - Regresja: MAE od k', x='k', y='MAE') + scale_color_discrete(name = "Implementacja", labels = c("Wlasna", "Biblioteka R")) + theme(legend.position = "bottom")
ggplot(VS_KNN_Reg , aes(x=k)) + geom_line(aes(y = MAET, color='green'), size=1, ) + geom_line(aes(y = MAEW, color='blue'), size=1, ) + geom_line(aes(y = MAE, color='red'), size=1,) +
labs(title='KNN - Regresja: MAE od k', x='k', y='MAE') + scale_color_discrete(name = "Implementacja", labels = c("Wlasna", "Biblioteka R")) + theme(legend.position = "bottom")
ggplot(VS_KNN_Reg , aes(x=k)) + geom_line(aes(y = MAET, color='green'), size=1, ) + geom_line(aes(y = MAEW, color='blue'), size=1, ) + geom_line(aes(y = MAE, color='red'), size=1,) +
labs(title='KNN - Regresja: MAE od k', x='k', y='MAE') + scale_color_discrete(name = "Implementacja", labels = c("Wlasna (T)", "Wlasna (W)","Biblioteka R")) + theme(legend.position = "bottom")
ggplot(VS_KNN_Reg , aes(x=k)) + geom_line(aes(y = MAET, color='green'), size=1, ) + geom_line(aes(y = MAEW, color='blue'), size=1, ) + geom_line(aes(y = MAE, color='red'), size=1,) +
labs(title='KNN - Regresja: MAE od k', x='k', y='MAE') + scale_color_discrete(name = "Implementacja", labels = c("Wlasna (W)", "Wlasna (T)","Biblioteka R")) + theme(legend.position = "bottom")
ggplot(VS_KNN_Reg , aes(x=k)) + geom_line(aes(y = MAET, color='green'), size=1, ) + geom_line(aes(y = MAEW, color='blue'), size=1, ) + geom_line(aes(y = MAE, color='red'), size=1,) +
labs(title='KNN - Regresja: MAE od k', x='k', y='MAE') + scale_color_discrete(name = "Implementacja:", labels = c("Wlasna (W)", "Wlasna (T)","Biblioteka R")) + theme(legend.position = "bottom")
VS_KNN_Bin <- data.frame(k = c(1:15))
VS_KNN_Bin <- merge(VS_KNN_Bin, KNN_bin_CrossValid_gr[,c('k', "JakoscT")], by = 'k')
VS_KNN_Bin <- merge(VS_KNN_Bin, KNN_bin_CrossValid_gr[,c('k', "JakoscW")], by = 'k')
VS_KNN_Bin <- merge(VS_KNN_Bin, KNN_bin_R_Wynik[,c('k', "Accuracy")], by = 'k')
print(VS_KNN_Bin)
ggplot(VS_KNN_Bin , aes(x=k)) + geom_line(aes(y = JakoscT, color='blue'), size=1, ) + geom_line(aes(y = JakoscW, color='blue'), size=1, ) + geom_line(aes(y = Accuracy, color='red'), size=1,) +
labs(title='KNN - Klas. Binarna: Jakosc od k', x='k', y='Jakosc') + scale_color_discrete(name = "Implementacja:", labels = c("Wlasna (W)", "Wlasna (T)","Biblioteka R")) + theme(legend.position = "bottom")
VS_KNN_Bin <- data.frame(k = c(1:15))
VS_KNN_Bin <- merge(VS_KNN_Bin, KNN_bin_CrossValid_gr[,c('k', "JakoscT")], by = 'k')
VS_KNN_Bin <- merge(VS_KNN_Bin, KNN_bin_CrossValid_gr[,c('k', "JakoscW")], by = 'k')
VS_KNN_Bin <- merge(VS_KNN_Bin, KNN_bin_R_Wynik[,c('k', "Accuracy")], by = 'k')
print(VS_KNN_Bin)
ggplot(VS_KNN_Bin , aes(x=k)) + geom_line(aes(y = JakoscT, color='green'), size=1, ) + geom_line(aes(y = JakoscW, color='blue'), size=1, ) + geom_line(aes(y = Accuracy, color='red'), size=1,) +
labs(title='KNN - Klas. Binarna: Jakosc od k', x='k', y='Jakosc') + scale_color_discrete(name = "Implementacja:", labels = c("Wlasna (W)", "Wlasna (T)", "Biblioteka R")) + theme(legend.position = "bottom")
VS_KNN_Multi <- data.frame(k = c(1:15))
VS_KNN_Multi <- merge(VS_KNN_Multi, KNN_multi_CrossValid_gr[,c('k', "JakoscT")], by = 'k')
VS_KNN_Multi <- merge(VS_KNN_Multi, KNN_multi_CrossValid_gr[,c('k', "JakoscW")], by = 'k')
VS_KNN_Multi <- merge(VS_KNN_Multi, KNN_multi_R_Wynik[,c('k', "Jakosc")], by = 'k')
VS_KNN_Multi <- merge(VS_KNN_Multi, KNN_multi_R_Wynik[,c('k', "Accuracy")], by = 'k')
print(VS_KNN_Multi)
ggplot(VS_KNN_Multi , aes(x=k)) + geom_line(aes(y = JakoscT, color='green'), size=1, ) + geom_line(aes(y = JakoscW, color='blue'), size=1, ) + geom_line(aes(y = Accuracy, color='red'), size=1,) +
labs(title='KNN - Klas. Wieloklasowa: Jakosc od k', x='k', y='Jakosc') + scale_color_discrete(name = "Implementacja:", labels = c("Wlasna (W)", "Wlasna (T)","Biblioteka R")) + theme(legend.position = "bottom")
ggplot(VS_KNN_Reg , aes(x=k)) + geom_line(aes(y = MAET, color='green'), size=1, ) + geom_line(aes(y = MAEW, color='blue'), size=1, ) + geom_line(aes(y = MAE, color='red'), size=1,) +
labs(title='KNN - Regresja: MAE od k', x='k', y='MAE') + scale_color_discrete(name = "Implementacja:", labels = c("Wlasna (W)", "Wlasna (T)","Biblioteka R")) + theme(legend.position = "bottom")
ggplot(VS_KNN_Bin , aes(x=k)) + geom_line(aes(y = JakoscT, color='green'), size=1, ) + geom_line(aes(y = JakoscW, color='blue'), size=1, ) + geom_line(aes(y = Accuracy, color='red'), size=1,) +
labs(title='KNN - Klas. Binarna: Jakosc od k', x='k', y='Jakosc') + scale_color_discrete(name = "Implementacja:", labels = c("Wlasna (W)", "Wlasna (T)", "Biblioteka R")) + theme(legend.position = "bottom")
ggplot(VS_KNN_Multi , aes(x=k)) + geom_line(aes(y = JakoscT, color='green'), size=1, ) + geom_line(aes(y = JakoscW, color='blue'), size=1, ) + geom_line(aes(y = Accuracy, color='red'), size=1,) +
labs(title='KNN - Klas. Wieloklasowa: Jakosc od k', x='k', y='Jakosc') + scale_color_discrete(name = "Implementacja:", labels = c("Wlasna (W)", "Wlasna (T)","Biblioteka R")) + theme(legend.position = "bottom")
parTune_Tree_reg <- expand.grid(type = c('SS'), depth = c(3,6,9), minobs = c(2,5,10), overfit = c('none'), cf = c(0.02))
CV_Tree_reg <- CrossValidTune(regresja_Y, regresja_X, regresja, algo = 'Tree', kFold = 10, parTune = parTune_Tree_reg, seed = 333)
Tree_reg_CrossValid_gr <- CV_Tree_reg %>% group_by(depth, minobs)
Tree_reg_CrossValid_gr[is.na(Tree_reg_CrossValid_gr)] <- 0
Tree_reg_CrossValid_gr <- as.data.frame(Tree_reg_CrossValid_gr %>% summarise(MAET = mean(MAET), MSET = mean(MSET), MAPET = mean(MAPET), MAEW = mean(MAEW), MSEW = mean(MSEW), MAPEW = mean(MAPEW)))
print(Tree_reg_CrossValid_gr)
Tree_reg_best_T <- Tree_reg_CrossValid_gr[which.min(Tree_reg_CrossValid_gr$MAET),]
Tree_reg_best_W <- Tree_reg_CrossValid_gr[which.min(Tree_reg_CrossValid_gr$MAEW),]
print(Tree_reg_best_T)
print(Tree_reg_best_W)
ggplot(Tree_reg_CrossValid_gr , aes(x=1:nrow(Tree_reg_CrossValid_gr))) +
geom_line(aes(y = MAET, color='red'), size=1, ) +
geom_line(aes(y = MAEW, color='blue'), size=1,) +
labs(title='Tree - Regresja: MAE dla kazdego modelu', x='ID modelu', y='MAE') +
scale_color_discrete(name = "Predykcja na zbiorze: ", labels = c("Treningowym", "Walidacyjnym")) +
theme(legend.position = "bottom")
ggplot(Tree_reg_CrossValid_gr , aes(x=1:nrow(Tree_reg_CrossValid_gr))) +
geom_line(aes(y = MAPET, color='red'), size=1, ) +
geom_line(aes(y = MAPEW, color='blue'), size=1,) +
labs(title='Tree - Regresja: MAPE dla kazdego modelu', x='ID modelu', y='MAPE') +
scale_color_discrete(name = "Predykcja na zbiorze: ", labels = c("Treningowym", "Walidacyjnym")) +
theme(legend.position = "bottom")
View(Tree_reg_CrossValid_gr)
View(binarna)
View(KNN_reg_best_T)
View(KNN_reg_best_W)
(KNN_reg_best_T)
(KNN_reg_best_W)
View(Tree_reg_best_W)
knn_grid_multi = expand.grid(k=2:15)
KNN_multi_R = train(x=wieloklasowa[,wieloklasowa_X], y=wieloklasowa[,wieloklasowa_Y], tuneGrid=knn_grid_multi, method='knn', metric='Accuracy', trControl=cv_R)
KNN_multi_R_Wynik = KNN_multi_R$results
print(paste("KNN w R - multi: k = ", KNN_multi_R$finalModel$k, " | Jakosc = " ,KNN_multi_R_Wynik$Accuracy[KNN_multi_R_Wynik$k == KNN_multi_R$finalModel$k]))
print(summary(regresja))
print(summary(wieloklasowa))
