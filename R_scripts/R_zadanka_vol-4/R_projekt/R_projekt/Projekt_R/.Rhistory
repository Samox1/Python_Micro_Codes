}
}
else if(is.factor(X_znormalizowane[,k]) & is.ordered(X_znormalizowane[,k]))
{
z_i <- (i - 1) / (n_wierszy_model - 1)
z_n <- (j - 1) / (n_wierszy_znorm - 1)
temp <- temp + (abs(z_i - z_n) / (n_wierszy_model - 1))
}
}
odleglosc[i, j] <- temp / n_kolumn_znorm
}
}
}
if(is.numeric(KNNmodel$y))
{
predykcja <- double(n_kolumn_znorm)
for(i in 1:n_wierszy_znorm)
{
k_najblizej <- order(odleglosc[,i])[1:KNNmodel$k]
y_predykcja <- mean(KNNmodel$y[k_najblizej])
predykcja[i] <- y_predykcja
}
return(predykcja)
}
else if(is.factor(KNNmodel$y))
{
predykcja <- as.data.frame(matrix(nrow = n_wierszy_znorm, ncol = nlevels(KNNmodel$y)+1))
for(i in 1:n_wierszy_znorm)
{
k_najblizej <- order(odleglosc[,i])[1:KNNmodel$k]
if(nlevels(KNNmodel$y) == 2)
{
pozytywna <- sum(KNNmodel$y[k_najblizej] == 1) / KNNmodel$k
negatywna <- sum(KNNmodel$y[k_najblizej] == 0) / KNNmodel$k
predykcja_klasy <- ifelse(pozytywna >= 0.5, 'P', 'N')
names(predykcja) <- c('P', 'N', 'Klasa')
predykcja[i, 1] <- pozytywna
predykcja[i, 2] <- negatywna
predykcja[i, 3] <- predykcja_klasy
}
else if(nlevels(KNNmodel$y) > 2)
{
etykiety <- sort(unique(KNNmodel$y))
names(predykcja) <- etykiety
names(predykcja)[nlevels(KNNmodel$y)+1] <- 'Klasa'
for (j in 1:length(etykiety))
{
pozytywna <- sum(KNNmodel$y[k_najblizej] == as.character(etykiety[j])) / KNNmodel$k
predykcja[i,j] <- pozytywna
}
predykcja_klasy <- etykiety[which.max(predykcja[i,])]
predykcja[i,'Klasa'] <- as.factor(predykcja_klasy)
}
}
return(predykcja)
}
else
{
stop("Dane y modelu sa niepoprawne!")
}
}
}
library(caret)
iris_test <- as.data.frame(iris)
iris_test$Species <- as.factor(iris_test$Species)
wiersze <- sample(nrow(iris_test), 125, replace = FALSE)
iris_train <- iris_test[wiersze,]
iris_pred <- iris_test[-wiersze,]
KNN_model <- KNNtrain(iris_train[,-5], iris_train[,5], k = 2, 0, 1)
test_1 <- cbind(test_1, KNNpred(KNN_model, iris_pred[,-5]))
KNN_model_pakiet <- knn3( iris_train[,-5], iris_train[,5], k = 2 )
test_1 <- predict( KNN_model_pakiet, iris_pred[,-5] )
test_1 <- cbind(test_1, '|')
KNN_model <- KNNtrain(iris_train[,-5], iris_train[,5], k = 2, 0, 1)
test_1 <- cbind(test_1, KNNpred(KNN_model, iris_pred[,-5]))
test_1 <- cbind(test_1, TRUE_Y = iris_pred[,5])
test_1
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
# parTune <- expand.grid(depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# parTune1 <- expand.grid(k=c(1:2), depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# # parTune1[parTune1[,colnames(parTune)] == parTune[3,]]
#
# kappa <- CrossValidTune(dane, X, Y, kFold = 2, parTune, algorytm="KNN", seed = 123)
### DANE ###
# Klasyfikacja Binarna      = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
# Klasyfikacja Wieloklasowa =
# Regresja                  =
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
### KNN ###
# ramka$k[id_modele], ramka$XminNew[id_modele], ramka$XmaxNew[id_modele]
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
# parTune <- expand.grid(depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# parTune1 <- expand.grid(k=c(1:2), depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# # parTune1[parTune1[,colnames(parTune)] == parTune[3,]]
#
# kappa <- CrossValidTune(dane, X, Y, kFold = 2, parTune, algorytm="KNN", seed = 123)
### DANE ###
# Klasyfikacja Binarna      = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
# Klasyfikacja Wieloklasowa =
# Regresja                  =
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
### KNN ###
# ramka$k[id_modele], ramka$XminNew[id_modele], ramka$XmaxNew[id_modele]
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[c(40:120),-5])
model <- KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[c(40:120),-5])
View(predykcja)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
model <- KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[c(40:120),-5])
predykcja <- KNNpred(model, dane[,-5])
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
model <- KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[,-5])
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
View(KNN_multi_CrossValid)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
model <- KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[,-5])
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
View(parTune_KNN_multi)
View(KNN_multi_CrossValid)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
View(KNN_multi_CrossValid)
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data")
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data")
View(dane_reg)
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", header = FALSE)
View(dane)
View(dane_reg)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", header = FALSE)
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-9]
dane_bin_Y <- bin_kolumny[9]
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", header = FALSE)
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-9]
dane_reg_Y <- reg_kolumny[9]
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
# parTune <- expand.grid(depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# parTune1 <- expand.grid(k=c(1:2), depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# # parTune1[parTune1[,colnames(parTune)] == parTune[3,]]
#
# kappa <- CrossValidTune(dane, X, Y, kFold = 2, parTune, algorytm="KNN", seed = 123)
### DANE ###
# Klasyfikacja Binarna      = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
# Klasyfikacja Wieloklasowa =
# Regresja                  =
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
### KNN ###
# ramka$k[id_modele], ramka$XminNew[id_modele], ramka$XmaxNew[id_modele]
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
# parTune <- expand.grid(depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# parTune1 <- expand.grid(k=c(1:2), depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# # parTune1[parTune1[,colnames(parTune)] == parTune[3,]]
#
# kappa <- CrossValidTune(dane, X, Y, kFold = 2, parTune, algorytm="KNN", seed = 123)
### DANE ###
# Klasyfikacja Binarna      = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
# Klasyfikacja Wieloklasowa =
# Regresja                  =
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
### KNN ###
# ramka$k[id_modele], ramka$XminNew[id_modele], ramka$XmaxNew[id_modele]
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00257/Data_User_Modeling_Dataset_Hamdi%20Tolga%20KAHRAMAN.xls")
View(dane_multi)
#Klasyfikacja
y_tar_klas <- as.factor(c(0,1,0,1,0,1,0,1,0,0,0,1,0,1,0,1,0,1,0,0))
y_hat_klas <- (length(y_tar_klas):1) / length(y_tar_klas)
iris_test <- as.data.frame(iris)
iris_test$Species <- as.factor(iris_test$Species)
wiersze <- sample(nrow(iris_test), 125, replace = FALSE)
iris_train <- iris_test[wiersze,]
iris_pred <- iris_test[-wiersze,]
KNN_model_pakiet <- knn3( iris_train[,-5], iris_train[,5], k = 2 )
test_1 <- predict( KNN_model_pakiet, iris_pred[,-5] )
KNN_model <- KNNtrain(iris_train[,-5], iris_train[,5], k = 2, 0, 1)
KNNpred(KNN_model, iris_pred[,-5])
KNNpred(KNN_model, iris_pred[,-5])&Klasa
KNNpred(KNN_model, iris_pred[,-5])$Klasa
is.factor(KNNpred(KNN_model, iris_pred[,-5])$Klasa)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00257/Data_User_Modeling_Dataset_Hamdi%20Tolga%20KAHRAMAN.xls")
View(dane_multi)
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data")
View(dane_multi)
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
View(dane_multi)
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
multi_kolumny <- colnames(dane_multi)
dane_multi_X <- multi_kolumny[-7]
dane_multi_Y <- multi_kolumny[7]
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
library(openxlsx)
source("funkcje.R")
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
View(dane_reg)
dane_reg <- dane_reg[,-10]
View(dane_reg)
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
multi_kolumny <- colnames(dane_multi)
dane_multi_X <- multi_kolumny[-7]
dane_multi_Y <- multi_kolumny[7]
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
dane_reg <- dane_reg[,-10]
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-7]
dane_reg_Y <- reg_kolumny[7]
dane_reg <- dane_reg[,-10]
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
dane_reg <- dane_reg[,-10]
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-9]
dane_reg_Y <- reg_kolumny[9]
### KNN ###
parTune_KNN_bin <- expand.grid(k=2)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
parTune_KNN_multi <- expand.grid(k=2)
KNN_multi_CrossValid <- CrossValidTune(dane_multi, dane_multi_X, dane_multi_Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
View(parTune_Tree_bin)
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
library(openxlsx)
source("funkcje.R")
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
multi_kolumny <- colnames(dane_multi)
dane_multi_X <- multi_kolumny[-7]
dane_multi_Y <- multi_kolumny[7]
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
dane_reg <- dane_reg[,-10]
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-9]
dane_reg_Y <- reg_kolumny[9]
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
View(parTune_Tree_bin)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
library(openxlsx)
source("funkcje.R")
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
multi_kolumny <- colnames(dane_multi)
dane_multi_X <- multi_kolumny[-7]
dane_multi_Y <- multi_kolumny[7]
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
dane_reg <- dane_reg[,-10]
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-9]
dane_reg_Y <- reg_kolumny[9]
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
Tree(dane_bin_Y, dane_bin_X, dane_bin, 'Entropy', 6, 2, 'none', 0.2)
