rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
# parTune <- expand.grid(depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# parTune1 <- expand.grid(k=c(1:2), depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# # parTune1[parTune1[,colnames(parTune)] == parTune[3,]]
#
# kappa <- CrossValidTune(dane, X, Y, kFold = 2, parTune, algorytm="KNN", seed = 123)
### DANE ###
# Klasyfikacja Binarna      = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
# Klasyfikacja Wieloklasowa =
# Regresja                  =
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
### KNN ###
# ramka$k[id_modele], ramka$XminNew[id_modele], ramka$XmaxNew[id_modele]
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
# parTune <- expand.grid(depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# parTune1 <- expand.grid(k=c(1:2), depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# # parTune1[parTune1[,colnames(parTune)] == parTune[3,]]
#
# kappa <- CrossValidTune(dane, X, Y, kFold = 2, parTune, algorytm="KNN", seed = 123)
### DANE ###
# Klasyfikacja Binarna      = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
# Klasyfikacja Wieloklasowa =
# Regresja                  =
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
### KNN ###
# ramka$k[id_modele], ramka$XminNew[id_modele], ramka$XmaxNew[id_modele]
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[c(40:120),-5])
model <- KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[c(40:120),-5])
View(predykcja)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
model <- KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[c(40:120),-5])
predykcja <- KNNpred(model, dane[,-5])
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
model <- KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[,-5])
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
View(KNN_multi_CrossValid)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
model <- KNNtrain(dane[,X], dane[,Y], 3, 0, 1)
predykcja <- KNNpred(model, dane[,-5])
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
View(parTune_KNN_multi)
View(KNN_multi_CrossValid)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane <- iris
nazwy_kolumn <- colnames(dane)
X <- nazwy_kolumn[-5]
Y <- nazwy_kolumn[5]
parTune_KNN_multi <- expand.grid(k=4)
KNN_multi_CrossValid <- CrossValidTune(dane, X, Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
View(KNN_multi_CrossValid)
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data")
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data")
View(dane_reg)
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", header = FALSE)
View(dane)
View(dane_reg)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", header = FALSE)
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-9]
dane_bin_Y <- bin_kolumny[9]
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", header = FALSE)
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-9]
dane_reg_Y <- reg_kolumny[9]
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
# parTune <- expand.grid(depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# parTune1 <- expand.grid(k=c(1:2), depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# # parTune1[parTune1[,colnames(parTune)] == parTune[3,]]
#
# kappa <- CrossValidTune(dane, X, Y, kFold = 2, parTune, algorytm="KNN", seed = 123)
### DANE ###
# Klasyfikacja Binarna      = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
# Klasyfikacja Wieloklasowa =
# Regresja                  =
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
### KNN ###
# ramka$k[id_modele], ramka$XminNew[id_modele], ramka$XmaxNew[id_modele]
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
# parTune <- expand.grid(depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# parTune1 <- expand.grid(k=c(1:2), depth=c(3:4), minobs=c(2:3), type=c('Gini', 'Entropy'), cf=0.2)
# # parTune1[parTune1[,colnames(parTune)] == parTune[3,]]
#
# kappa <- CrossValidTune(dane, X, Y, kFold = 2, parTune, algorytm="KNN", seed = 123)
### DANE ###
# Klasyfikacja Binarna      = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
# Klasyfikacja Wieloklasowa =
# Regresja                  =
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
### KNN ###
# ramka$k[id_modele], ramka$XminNew[id_modele], ramka$XmaxNew[id_modele]
parTune_KNN_bin <- expand.grid(k=4)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00257/Data_User_Modeling_Dataset_Hamdi%20Tolga%20KAHRAMAN.xls")
View(dane_multi)
#Klasyfikacja
y_tar_klas <- as.factor(c(0,1,0,1,0,1,0,1,0,0,0,1,0,1,0,1,0,1,0,0))
y_hat_klas <- (length(y_tar_klas):1) / length(y_tar_klas)
iris_test <- as.data.frame(iris)
iris_test$Species <- as.factor(iris_test$Species)
wiersze <- sample(nrow(iris_test), 125, replace = FALSE)
iris_train <- iris_test[wiersze,]
iris_pred <- iris_test[-wiersze,]
KNN_model_pakiet <- knn3( iris_train[,-5], iris_train[,5], k = 2 )
test_1 <- predict( KNN_model_pakiet, iris_pred[,-5] )
KNN_model <- KNNtrain(iris_train[,-5], iris_train[,5], k = 2, 0, 1)
KNNpred(KNN_model, iris_pred[,-5])
KNNpred(KNN_model, iris_pred[,-5])&Klasa
KNNpred(KNN_model, iris_pred[,-5])$Klasa
is.factor(KNNpred(KNN_model, iris_pred[,-5])$Klasa)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
source("funkcje.R")
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00257/Data_User_Modeling_Dataset_Hamdi%20Tolga%20KAHRAMAN.xls")
View(dane_multi)
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data")
View(dane_multi)
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
View(dane_multi)
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
multi_kolumny <- colnames(dane_multi)
dane_multi_X <- multi_kolumny[-7]
dane_multi_Y <- multi_kolumny[7]
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
library(openxlsx)
source("funkcje.R")
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
View(dane_reg)
dane_reg <- dane_reg[,-10]
View(dane_reg)
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
multi_kolumny <- colnames(dane_multi)
dane_multi_X <- multi_kolumny[-7]
dane_multi_Y <- multi_kolumny[7]
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
dane_reg <- dane_reg[,-10]
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-7]
dane_reg_Y <- reg_kolumny[7]
dane_reg <- dane_reg[,-10]
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
dane_reg <- dane_reg[,-10]
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-9]
dane_reg_Y <- reg_kolumny[9]
### KNN ###
parTune_KNN_bin <- expand.grid(k=2)
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_KNN_bin, algorytm="KNN", seed = 123)
parTune_KNN_multi <- expand.grid(k=2)
KNN_multi_CrossValid <- CrossValidTune(dane_multi, dane_multi_X, dane_multi_Y, kFold = 1, parTune_KNN_multi, algorytm="KNN", seed = 123)
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
View(parTune_Tree_bin)
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
library(openxlsx)
source("funkcje.R")
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
multi_kolumny <- colnames(dane_multi)
dane_multi_X <- multi_kolumny[-7]
dane_multi_Y <- multi_kolumny[7]
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
dane_reg <- dane_reg[,-10]
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-9]
dane_reg_Y <- reg_kolumny[9]
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
View(parTune_Tree_bin)
rm(list=ls())
library(data.tree)
library(rpart.plot)
library(tidyverse)
library(pROC)
library(openxlsx)
source("funkcje.R")
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", header = FALSE)
multi_kolumny <- colnames(dane_multi)
dane_multi_X <- multi_kolumny[-7]
dane_multi_Y <- multi_kolumny[7]
dane_multi[,7] <- as.factor(dane_multi[,7])
dane_reg <- read.xlsx("https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx")
dane_reg <- dane_reg[,-10]
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-9]
dane_reg_Y <- reg_kolumny[9]
### Drzewa Decyzyjne ###
parTune_Tree_bin <- expand.grid(depth=c(6), minobs=c(2), type=c('Entropy'), overfit = c('none'), cf=0.2)
Tree_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 1, parTune_Tree_bin, algorytm="Tree", seed = 123)
Tree(dane_bin_Y, dane_bin_X, dane_bin, 'Entropy', 6, 2, 'none', 0.2)
rm(list=ls())
rm(list=ls())
library(openxlsx)
source("funkcje.R")
### DANE ###
# Klasyfikacja Binarna      = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers         # Dane wybrane przez prowadzacego
# Klasyfikacja Wieloklasowa = https://archive.ics.uci.edu/ml/datasets/seeds
# Regresja                  = https://archive.ics.uci.edu/ml/datasets/Computer+Hardware
dane_bin <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv")
dane_bin <- dane_bin[,-2]
bin_kolumny <- colnames(dane_bin)               # glupi blad - przy zebraniu nazwy "dane_bin[,1]" jest NULL
dane_bin_X <- bin_kolumny[-1]
dane_bin_Y <- bin_kolumny[1]
dane_bin[,1] <- as.factor(dane_bin[,1])
print("*** Dane - klasyfikacja binarna ***")
print(head(dane_bin))
print("*********************************")
dane_multi <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt", header = FALSE, sep = "\t")
dane_multi <- drop_na(dane_multi)
multi_kolumny <- colnames(dane_multi)
dane_multi_X <- multi_kolumny[-8]
dane_multi_Y <- multi_kolumny[8]
dane_multi[,8] <- as.factor(dane_multi[,8])
print("*** Dane - klasyfikacja multi ***")
print(head(dane_multi))
print("*********************************")
dane_reg <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data", header = FALSE)
dane_reg <- dane_reg[,-c(2,10)]
dane_reg[,1] <- as.numeric(dane_reg[,1])
reg_kolumny <- colnames(dane_reg)
dane_reg_X <- reg_kolumny[-8]
dane_reg_Y <- reg_kolumny[8]
print("*** Dane - regresja ***")
print(head(dane_reg))
print("*********************************")
cat(" \n")
print("//////////////////////////////////////////////////////////")
print("/////////////////////// OBLICZENIA ///////////////////////")
print("//////////////////////////////////////////////////////////")
### KNN ###
print("*** KNN - bin - kroswalidacja ***")
parTune_KNN_bin <- expand.grid(k=c(2:15))
KNN_bin_CrossValid <- CrossValidTune(dane_bin, dane_bin_X, dane_bin_Y, kFold = 10, parTune_KNN_bin, algorytm="KNN", seed = 123)
# KNN_bin_CrossValid
KNN_bin_CrossValid
KNN_bin_CrossValid_gr <- KNN_bin_CrossValid %>% group_by(k)
KNN_bin_CrossValid_gr <- as.data.frame(KNN_bin_CrossValid_gr %>% summarise(
AUCT = mean(AUCT), CzuloscT = mean(CzuloscT), SpecyficznoscT = mean(SpecyficznoscT), JakoscT = mean(JakoscT),
AUCW = mean(AUCW), CzuloscW = mean(CzuloscW), SpecyficznoscW = mean(SpecyficznoscW), JakoscW = mean(JakoscW), ))
print(KNN_bin_CrossValid_gr)
KNN_bin_best_T <- KNN_bin_CrossValid_gr[which.max(KNN_bin_CrossValid_gr$JakoscT),]
KNN_bin_best_W <- KNN_bin_CrossValid_gr[which.max(KNN_bin_CrossValid_gr$JakoscW),]
print(KNN_bin_best_T)
print(KNN_bin_best_W)
print("KNN - R - bin")
knn_grid_bin = expand.grid(k=2:50)
KNN_bin_R = train(x=dane_bin[,dane_bin_X], y=dane_bin[,dane_bin_Y], tuneGrid=knn_grid_bin, method='knn', metric='Accuracy', trControl=cv_R)
KNN_bin_R_Wynik = KNN_bin_R$results
print(paste("Najlepszy KNN w R - Binarny: k = ", KNN_bin_R$finalModel$k, " | Accuracy = " ,KNN_bin_R_Wynik$Accuracy[KNN_bin_R_Wynik$k == KNN_bin_R$finalModel$k]))
cv_R <- trainControl(method="cv", number=10)
print("KNN - R - bin")
knn_grid_bin = expand.grid(k=2:50)
KNN_bin_R = train(x=dane_bin[,dane_bin_X], y=dane_bin[,dane_bin_Y], tuneGrid=knn_grid_bin, method='knn', metric='Accuracy', trControl=cv_R)
KNN_bin_R_Wynik = KNN_bin_R$results
print(paste("Najlepszy KNN w R - Binarny: k = ", KNN_bin_R$finalModel$k, " | Accuracy = " ,KNN_bin_R_Wynik$Accuracy[KNN_bin_R_Wynik$k == KNN_bin_R$finalModel$k]))
Porownanie_KNN_Bin <- data.frame(k = c(1:20))
Porownanie_KNN_Bin <- merge(Porownanie_KNN_Bin, KNN_bin_CrossValid_gr[,c('k', "JakoscW")], by = 'k')
Porownanie_KNN_Bin <- merge(Porownanie_KNN_Bin, KNN_bin_R_Wynik[,c('k', "Accuracy")], by = 'k')
print(Porownanie_KNN_Bin)
ggplot(Porownanie_KNN_Bin , aes(x=k)) +
geom_line(aes(y = JakoscW, color='blue'), size=1, ) +
geom_line(aes(y = Accuracy, color='red'), size=1,) +
labs(title='KNN - Klas. Binarna: Accuracy od k', x='k', y='Accuracy') +
scale_color_discrete(name = "Implementacja", labels = c("Wlasna", "Biblioteka R")) +
theme(legend.position = "bottom")
print(KNN_bin_CrossValid_gr)
View(KNN_bin_CrossValid_gr)
