tree$Count <- nrow(data)
if (type == "Entropy") {
tree$inf <- Entropy(Prob(data[,Y]))
}
else if (type == "Gini") {
tree$inf <- Gini(Prob(data[,Y]))
}
else {
tree$inf <- SS(data[,Y])
}
tree <- AssignInitialMeasures(tree, Y, data, type, depth)
BuildTree( tree, Y, X, data, depth, type, minobs)
PruneTree <- function(){}
AssignInfo(tree,Y,X,data,type,depth, minobs, overfit, cf )
return( tree )
}
# Zadanie 3:
# a) Stw?rz funkcj? "PredictTree" przyjmuj?c? nast?uj?ce parametry: "tree", "data".
# b) Funkcja w pierwszym kroku powinna sprawdza? czy przewidywanie zmiennej celu dla nowego zbioru danych jest mo?liwe do wykonania,
#    tj. czy wszystkie zmienne, kt?re buduj? struktur? drzewa istniej? w nowym zbiorze danych
#        oraz czy wszystkie kategorie dla zmiennych porz?dkowych i nominalnych istniej? w nowym zbiorze danych.
# c) Funkcja powinna rekurencyjnie przechodzi? po strukturze drzewa i wykonywa? testy w ka?dym w??le dla danego atrybutu i punktu podzia?u.
#    Przechodz?c do finalnego li?cia funkcja powinna odczytywa? warto?? prognozowan?.
# d) Funkcja powinna zwraca?:
#    - dla regresji: wektor z warto?ciami przewidywanymi.
#    - dla klasyfikacji: nazwan? ramk? danych o rozmiarze "n x k+1", np. dla wersji binarnej o etykietach "P", "N",
#      tabela wygl?da nast?puj?co: data.frame( P = c(0.3,0.6), N = c(0.7,0.4), Klasa = c("N","P") ),
#      tj. pierwsze dwie kolumny zawieraj? prawdopodobie?stwo przynale?no?ci danej obserwacji do danej klasy (nazwy kolumn s? wa?ne),
library(data.tree)
ObsPred <- function(tree, obs) {
if (tree$isLeaf) {
print(tree$Prob)
return(data.frame("Prob" = max(tree$Prob), "Class" = (tree$Class), stringsAsFactors = F))}
if (is.numeric(tree$children[[1]]$BestSplit) | is.ordered(tree$children[[1]]$BestSplit)) {
child <- tree$children[[ifelse(obs[,tree$children[[1]]$feature] > (tree$children[[1]]$BestSplit), 2, 1)]]}
else {
split <- tree$children[[1]]$feature
child <- tree$children[[ifelse((obs[,tree$children[[1]]$feature] %in% split), 1, 2)]]}
return (ObsPred(child,obs))
}
PredictTree <- function(tree, data) {
if (is.factor(attributes(tree)$Y)) {
res <- data.frame(matrix(0, nrow = nrow(data), ncol = 2))
for (i in 1:nrow(data)) {
res[i,] <- (ObsPred(tree, data[i, ,drop = F]))}
colnames(res) <- c("Prob", "Class")
}
else{
res <- c()
for (i in 1:nrow(data)) {
res[i] <- (ObsPred(tree, data[i, ,drop = F])$Class)}
}
return (res)
}
iris
head(iris)
Tree1 <- Tree(colnames(iris[,5]), colnames(iris[,-5]) , iris, 'Gini', 6, 2, 0.1, 0.1)
Tree1 <- Tree(colnames(iris[,5]), colnames(iris[,-5]) , iris, 'Gini', 6, 2, 'none', 0.1)
colnames(iris[,-5])
Tree1 <- Tree("Species", c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width") , iris, 'Gini', 6, 2, 'none', 0.1)
print(Tree1)
PredictTree(Tree1, iris[5,-5])
PredictTree(Tree1, iris[100,-5])
PredictTree(Tree1, iris[78,-5])
iris[,5] <- as.numeric(iris[,5])
iris
is.factor(iris[,5])
Tree1 <- Tree("Species", c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width") , iris, 'Gini', 6, 2, 'none', 0.1)
Tree1 <- Tree("Species", c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width") , iris, 'SS', 6, 2, 'none', 0.1)
print(Tree1)
PredictTree(Tree1, iris[78,-5])
print(Tree1, "Count", "Class", "Prob", "Leaf", "Depth")
### Funkcje potrzebne do dzialania drzewa
StopIfNot <- function(Y, X, data, type, depth, minobs, overfit, cf){
war1 <- is.data.frame(data)
war2 <- all(is.element(Y, colnames(data)))
war3 <- all(is.element(X, colnames(data)))
if(war2 && war3)
{
war4 <- !any(is.na(data[,Y]))
war5 <- !any(is.na(data[,X]))
war11 <- (is.numeric(data[,Y]) && type=="SS") || (is.factor(data[,Y]) && type=="Gini") || (is.factor(data[,Y]) && type == "Entropy")
}
else{
warning("Nie ma wskazanych kolumn")
return(FALSE)
}
war6 <- depth > 0
war7 <- minobs > 0
war8 <- (type == "Gini" || type == "SS" || type == "Entropy")
war9 <- (overfit == "none" || overfit =="prune")
war10 <- cf > 0 && cf <= 0.5
if(war1 == FALSE){
warning("ramka danych nie jest typu 'data.frame'")
}
if(war4 == FALSE){
warning("kolumny 'Y' -> braki danych")
}
if(war5 == FALSE){
warning("kolumny 'X' -> braki danych")
}
if(war6 == FALSE){
warning("parametr 'depth' powinien byc wiekszy od 0")
}
if(war7 == FALSE){
warning("parametr 'minobs' powinien byc wiekszy od 0")
}
if(war8 == FALSE){
warning("parametr 'type' moze przyjmowac wartosci: 'Gini', 'Entropy', 'SS'")
}
if(war9 == FALSE){
warning("parametr 'overfit' powinien byc: 'none' lub 'prune'")
}
if(war10 == FALSE){
warning("parametr 'cf' zawiera sie w przedziale: (0,0.5]")
}
if(war11 == FALSE){
warning("kombinacja 'Y' i parametru 'type' nie ma sensu")
}
if(war1 && war2 && war3 && war4 && war5 && war6 && war7 && war8 && war9 && war10 && war11)
{
return(TRUE)
}
else
{
return(FALSE)
}
}
Prob <- function( y ){
res <- unname( table( y ) )
res <- res / sum( res )
return( res )
}
Entropy <- function( prob ){
res <- prob * log2( prob )
res[ prob == 0 ] <- 0
res <- -sum( res )
return( res )
}
Gini <- function(prob){
res <- prob*prob
res <- sum(res)
res <- 1-res
return(res)
}
SS <- function(y){
n<-length(y)
y_hat <- sum(y)/n
res <- y-y_hat
res <- res*res
res <- sum(res)
return(res)
}
AssignInitialMeasures <- function(tree, Y, data, type, depth){
tree$Depth <- 0
if(type == "Gini")
{
pr <- Prob(data[,Y])
value <- Gini(pr)
tree$Val <- value
}
else if(type == "SS")
{
value <- SS(data[,Y])
tree$Val <- value
}
else if(type == "Entropy")
{
pr <- Prob(data[,Y])
value <-Entropy(pr)
tree$Val <- value
}
return(tree)
}
AssignInfo <- function(tree, Y, X, data, type, depth, minobs, overfit, cf){
attr(tree, "Y") <- Y
attr(tree, "X") <- X
attr(tree, "data") <- data
attr(tree, "type") <- type
attr(tree, "depth") <- depth
attr(tree, "minobs") <- minobs
attr(tree, "overfit") <- overfit
attr(tree, "cf") <- cf
return(tree)
}
SplitNum <- function( Y, X, parentVal, splits, type, minobs ){
n <- length(X)
res <- data.frame(matrix(0, length(splits), 6))
colnames( res ) <- c("InfGain","lVal","rVal","point","ln","rn")
for( i in 1:length(splits)){
partition <- X <= splits[i]
ln <- sum(partition)
rn <- n - ln
if(any(c(ln,rn) < minobs)){
res[i,] <- 0
}else{
if(type=="Entropy"){
prob1 <- Prob(Y[partition])
prob2 <- Prob(Y[!partition])
lVal <- Entropy(prob1)
rVal <- Entropy(prob2)
}else if(type=="Gini"){
prob1 <- Prob(Y[partition])
prob2 <- Prob(Y[!partition])
lVal <- Gini(prob1)
rVal <- Gini(prob2)
}else if(type=="SS"){
lVal <- SS(Y[partition])
rVal <- SS(Y[!partition])
}
InfGain <- parentVal - ( ln/n * lVal + rn/n * rVal )
res[i,"InfGain"] <- InfGain
res[i,"lVal"] <- lVal
res[i,"rVal"] <- rVal
res[i,"point"] <- splits[ i ]
res[i,"ln"] <- ln
res[i,"rn"] <- rn
}
}
return( res )
}
SplitVar <- function( Y, X, parentVal, type, minobs ){
s <- unique(X)
if(length(X) == 1){
splits <- s
}else{
splits <- head(sort(s), -1)
}
res <- SplitNum(Y, X, parentVal, splits, type, minobs )
incl <- res$ln >= minobs & res$rn >= minobs & res$InfGain > 0
res <- res[incl, , drop = F]
best <- which.max( res$InfGain )
res <- res[best, , drop = F]
return( res )
}
# Zadanie 1:
# a) Stwórz funkcję "FindBestSplit" przyjmującą nastęujące parametry: "Y", "X", "data", "parentVal", "type", "minobs".
# b) Funkcja powinna zwracać tabelę z wynikami najlepszego możliwego podziału, zawierjącą:
#    - "infGain" - zysk informacyjny dla podziału,
#    - "lVal" - miarę niejednorodności dla lewego węzła,
#    - "rVal" - miarę niejednorodności dla prawego węzła,
#    - "point" - punkt (lub zbiór punktów dla zmiennych kategorycznych) podzału,
#    - "Ln" - liczbę obserwacji w lewym węźle,
#    - "Rn" - liczbę obserwacji w prawym węźle.
# c) Funkcja powinna akceptować zmienne ciagłe, porządkowe oraz nominalne. Dwa ostatnie typy reprezentpwane są jako factor.
FindBestSplit <- function( Y, Xnames, data, parentVal, type, minobs ){
if(is.numeric(data[,Y])){
for(col_var in Xnames){
if(!is.numeric(data[,col_var]) & !is.ordered(data[,col_var])){
tmp <- tapply(data[,Y], as.numeric(data[,col_var]), mean)
tmp <- sort(tmp)
data[,col_var] <- factor(data[,col_var], levels = names(tmp), ordered = TRUE)
}
}
}else{
for(col_var in Xnames){
if(!is.numeric(data[,col_var]) & !is.ordered(data[,col_var])){
positive_rows <- data[data[,Y]==levels(data[,Y])[1],]
tmp <- prop.table(table(positive_rows[,col_var]))
tmp <- sort(tmp)
data[,col_var] <- factor(data[,col_var], levels = names(tmp), ordered = TRUE)
}
}
}
res <- sapply( Xnames, function(i){
SplitVar(Y = data[,Y] , X = data[,i], parentVal = parentVal, type = type, minobs = minobs)
}, simplify = F)
res <- do.call(rbind, res)
best <- which.max(res$InfGain)
res <- res[ best, , drop = F]
return(res)
}
# Zadanie 2:
# a) Dokonaj integracji opracowanej funkcji "FindBestSplit" z funkcjami "Tree" oraz "BuildTree".
library( data.tree )
BuildTree <- function( node, Y, Xnames, data, type, depth, minobs ){
node$Count <- nrow( data )
node$Prob <- Prob( data[,Y] )
node$Class <- levels( data[,Y] )[ which.max(node$Prob) ]
bestSplit <- FindBestSplit( Y, Xnames, data, node$Val, type, minobs )
ifStop <- nrow( bestSplit ) == 0
if( node$Depth == depth | ifStop | all( node$Prob %in% c(0,1) ) ){
node$Leaf <- "*"
return( node )
}
splitIndx <- data[, rownames(bestSplit) ] <= bestSplit$point
node$BestAtr <- rownames(bestSplit)
node$SplitPoint <- bestSplit$point
childFrame <- split( data, splitIndx )
namel <- sprintf( "%s <= %s",  rownames(bestSplit), bestSplit$point )
childL <- node$AddChild( namel )
childL$Depth <- node$Depth + 1
childL$Val <- bestSplit$lVal
BuildTree( childL, Y, Xnames, childFrame[["TRUE"]], type, depth, minobs )
namer <- sprintf( "%s >  %s",  rownames(bestSplit), bestSplit$point )
childR <- node$AddChild( namer )
childR$Depth <- node$Depth + 1
childR$Val <- bestSplit$rVal
BuildTree( childR, Y, Xnames, childFrame[["FALSE"]], type, depth, minobs )
}
Tree <- function(Y, X, data, type, depth, minobs, overfit, cf){
if(StopIfNot (Y=Y, X=X, data=data, type=type, depth=depth, minobs=minobs, overfit=overfit, cf=cf) == FALSE){
stop("Zle dane lub parametry wejsciowe!")
}
tree <- Node$new("Root")
tree$Count <- nrow(data)
AssignInfo(tree, Y=Y, X=X, data=data, type=type, depth=depth, minobs=minobs, overfit=overfit, cf=cf)
AssignInitialMeasures(tree, Y=Y, data=data, type=type, depth=0)
BuildTree(tree, Y, X, data, type, depth, minobs)
return(tree)
}
Prediction <- function(tree, data)
{
if(tree$isLeaf){
return(c(tree$Prob, tree$Class))
}
if(data[,tree$BestAtr] <= tree$SplitPoint){
return(Prediction(tree$children[[1]], data))
}else{
return(Prediction(tree$children[[2]], data))
}
}
PredictTree<-function(tree, new_data)
{
if(all(colnames(new_data) %in% attributes(tree)$X) == FALSE )
{
warning("'Y' lub 'X' brakuje w danych")
return(1)
}
for(i in colnames(new_data))
{
if(is.factor(new_data[,i]) || is.ordered(new_data[,i]))
{
if(all(levels(new_data[,i]) %in% levels(attributes(tree)$data[,i])) == FALSE)
{
warning("Poziomy w danych do predykcji nie zgadzaja sie z danymi uczacymi")
return(1)
}
}
}
if(is.factor(attributes(tree)$data[,attributes(tree)$Y]))
{
Class_new <- c()
for(i in 1:nrow(new_data))
{
probability <- Prediction(tree, new_data[i,])
Class_new <- rbind(Class_new, probability)
}
Class_new <- data.frame(Class_new)
col_names <- c(levels(attributes(tree)$data[,attributes(tree)$Y]), "Klasa")
colnames(Class_new) <- col_names
return(Class_new)
}
else if(is.numeric(attributes(tree)$data[,attributes(tree)$Y]))
{
return("... regresja ...")
}
}
iris
head(iris)
iris[,5] <- as.numeric(iris[,5])
is.factor(iris[,5])
iris
Tree1 <- Tree("Species", c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width") , iris, 'SS', 6, 2, 'none', 0.1)
BuildTree <- function( node, Y, Xnames, data, type, depth, minobs ){
node$Count <- nrow( data )
if (is.factor(data[,Y])){
node$Prob <- Prob(data[,Y])
node$Class <- levels(data[,Y])[which.max(node$Prob)]
}
else{
node$Prob <- SS(data[,Y])
node$Class <- mean(data[,Y])
}
bestSplit <- FindBestSplit( Y, Xnames, data, node$Val, type, minobs )
ifStop <- nrow( bestSplit ) == 0
if( node$Depth == depth | ifStop | all( node$Prob %in% c(0,1) ) ){
node$Leaf <- "*"
return( node )
}
splitIndx <- data[, rownames(bestSplit) ] <= bestSplit$point
node$BestAtr <- rownames(bestSplit)
node$SplitPoint <- bestSplit$point
childFrame <- split( data, splitIndx )
namel <- sprintf( "%s <= %s",  rownames(bestSplit), bestSplit$point )
childL <- node$AddChild( namel )
childL$Depth <- node$Depth + 1
childL$Val <- bestSplit$lVal
BuildTree( childL, Y, Xnames, childFrame[["TRUE"]], type, depth, minobs )
namer <- sprintf( "%s >  %s",  rownames(bestSplit), bestSplit$point )
childR <- node$AddChild( namer )
childR$Depth <- node$Depth + 1
childR$Val <- bestSplit$rVal
BuildTree( childR, Y, Xnames, childFrame[["FALSE"]], type, depth, minobs )
}
iris
head(iris)
iris[,5] <- as.numeric(iris[,5])
is.factor(iris[,5])
iris
Tree1 <- Tree("Species", c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width") , iris, 'SS', 6, 2, 'none', 0.1)
r version
R -- version
R --version
R.version
x <- 1:10
plot(x, x^2)
plot (x,
x^2,
xlab = "x",
ylab = expression(f(xi)==x^2),
main = "Wykres funkcji f(x)",
col = "red",
pch = 19,
font = 2,
font.lab = 4,
font.main = 3,
cex = 2)
y <- 2*x+2
z <- 3*x-1
plot (x, y, type = "l", lwd = 2, col = "blue")
lines (x, z, lwd = 2, col = "red")
png ("fig2.png")
plot (x, y, type = "l", lwd = 2, col = "blue")
lines (x, z, lwd = 2, col = "red")
dev.off()
getwd()
install(here)
install here
install.packages(here)
install.packages('here')
here
here()
here::here()
here::i_am()
x <- 1:10
plot (x,
x^2,
xlab = "x",
ylab = expression(f(x)),
col = "red",
bg = "red",
pch = 21,
cex = 2)
points (x,
x^1.5,
col = "blue",
bg = "blue",
pch = 22,
cex = 2)
legend ("topleft",
legend = c (expression(x^1.5),expression(x^2)),
pch = c (22,19),
col = c ("blue","red"),
pt.bg = c("blue","red"))
y <- c(0,0,1,2,3,1,2,3,4)
tabulate(y)
y <- c(0,0,1.1,1.9,2.1,2,3)
tabulate(y)
df <- data.frame(x=c(1,1,2,2,3,4,5), y=c(2,2,3,1,5,5,5))
df
table(df)
x <- c(1,1,1,2,2,4,10)
hist(x)
x <- c (1,1,1,2,2,4,10)
h <- hist (x)
h
x <- rep (1:5, c(20, 9, 4, 0, 1))
h <- hist (x)
h <- hist (x, breaks = 0.5:5.5)
runif (5)
runif (5, -10, 10)
x <- seq (-2, 2, 0.1)
sigmas <- c (0.5, 1, 0.3)
plot (x, dnorm(x, 0, sigmas[1]), ylim = c(0,1.5), pch = 19)
points (x, dnorm(x, 0, sigmas[2]), pch = 19, col = "blue")
points (x, dnorm(x, 0, sigmas[3]), pch = 19, col = "green", t = "o")
lines (x, pnorm(x, 0, sigmas[3]), col = "red", lwd = 2)
legend ("topleft",
legend = c (expression (sigma==0.5),
expression (sigma==1.0),
expression (sigma==0.3),
expression (sigma==0.3)),
lty = c (0, 0, 1, 1),
pch = c (19, 19, 19, NA),
col = c ("black","blue", "green", "red")
)
qnorm (0.95, 0, 1)
qnorm (0.5, 3, 0.5)
# Random walker
sample(c(-1,1),100,replace=TRUE)
# Random walker
rw <- sample(c(-1,1),100,replace=TRUE)
plot(rw)
rw_cum <- cumsum(rw)
plot(rw_cum)
plot(rw_cum, type = "l")
x <- c(1,1,1,2,5,6,3,7,8,10)
plot(ecdf(x))
get.x <- function(x) {
return(seq(min(x), max(x), length.out = 100))
}
make.plots <- function(N) {
x <- rnorm(N, 0, 1)
plot(ecdf(x), main = N)
xx <- get.x(x)
lines(xx, pnorm(xx, 0, 1), col = "red", lwd = 2)
}
par(mfrow = c(2,2))
N <- c(10, 50, 100, 500)
sapply(N, make.plots)
setwd("E:/GitHub/Python_Micro_Codes/AWDJR_2022Z/Laboratorium_4")
